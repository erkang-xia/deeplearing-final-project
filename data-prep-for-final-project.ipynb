{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3919937,"sourceType":"datasetVersion","datasetId":2327240},{"sourceId":8272660,"sourceType":"datasetVersion","datasetId":4911947},{"sourceId":8318243,"sourceType":"datasetVersion","datasetId":4940714},{"sourceId":8348398,"sourceType":"datasetVersion","datasetId":4959670}],"dockerImageVersionId":30204,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:03.758123Z","iopub.execute_input":"2024-05-07T22:20:03.758739Z","iopub.status.idle":"2024-05-07T22:20:03.934955Z","shell.execute_reply.started":"2024-05-07T22:20:03.758615Z","shell.execute_reply":"2024-05-07T22:20:03.933335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport os\nfrom glob import glob\n\n# Assuming data is located in a directory accessible in this path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport os\nfrom glob import glob\n\nannot = pd.read_parquet('../input/textocr-text-extraction-from-images-dataset/annot.parquet')\nimgs = pd.read_parquet('../input/textocr-text-extraction-from-images-dataset/img.parquet')\nimg_fns = glob('../input/textocr-text-extraction-from-images-dataset/train_val_images/train_images/*')\nplt.style.use('ggplot')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:03.937286Z","iopub.execute_input":"2024-05-07T22:20:03.937688Z","iopub.status.idle":"2024-05-07T22:20:08.958988Z","shell.execute_reply.started":"2024-05-07T22:20:03.937652Z","shell.execute_reply":"2024-05-07T22:20:08.957548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, out_channels, apply_batchnorm=True):\n        super(Downsample, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=not apply_batchnorm)\n        self.batchnorm = nn.BatchNorm2d(out_channels) if apply_batchnorm else nn.Identity()\n        self.activation = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        return self.activation(x)\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, out_channels, apply_dropout=False):\n        super(Upsample, self).__init__()\n        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n        self.dropout = nn.Dropout(0.5) if apply_dropout else nn.Identity()\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        x = self.dropout(x)\n        return self.activation(x)\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.down_blocks = nn.ModuleList([\n            Downsample(3, 64, apply_batchnorm=False),\n            Downsample(64, 128),\n            Downsample(128, 256),\n            Downsample(256, 512),\n            Downsample(512, 512),\n            Downsample(512, 512),\n            Downsample(512, 512),\n            Downsample(512, 512)\n        ])\n        self.up_blocks = nn.ModuleList([\n            Upsample(512, 512, apply_dropout=True),\n            Upsample(1024, 512, apply_dropout=True),\n            Upsample(1024, 512, apply_dropout=True),\n            Upsample(1024, 512),\n            Upsample(1024, 256),\n            Upsample(512, 128),\n            Upsample(256, 64)\n        ])\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        connections = []\n        for down in self.down_blocks:\n            x = down(x)\n            connections.append(x)\n        connections = connections[:-1][::-1]\n        for up, conn in zip(self.up_blocks, connections):\n            x = up(x)\n            x = torch.cat([x, conn], 1)\n        return self.final(x)\n\nimport torch\n\n# Assuming generator and discriminator are instances of nn.Module\ngenerator = Generator()#.to(device)\ngenerator.load_state_dict(torch.load('/kaggle/input/checkpointforcganwithwhite/generator_with_white_epoch_100.pth',map_location=torch.device('cpu')))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:08.961371Z","iopub.execute_input":"2024-05-07T22:20:08.961919Z","iopub.status.idle":"2024-05-07T22:20:14.656112Z","shell.execute_reply.started":"2024-05-07T22:20:08.961866Z","shell.execute_reply":"2024-05-07T22:20:14.654782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n\ndef generate_images(generator, test_input, target, epoch, display=True):\n    generator.eval()  # Set the generator to evaluation mode\n    with torch.no_grad():\n        prediction = generator(test_input).to('cpu')\n\n    test_input = test_input.to('cpu')\n    target = target.to('cpu')\n    \n    # Each image type will have its own column, and images will be displayed in rows\n    fig, axs = plt.subplots(2, 3, figsize=(15, 40))  # 10 rows, 3 columns\n\n    for i in range(1):  # Assuming there are exactly 10 images in the batch\n        axs[i, 0].imshow(test_input[i].permute(1, 2, 0) * 0.5 + 0.5)\n        axs[i, 0].set_title('Input Image')\n        axs[i, 0].axis('off')\n\n        axs[i, 1].imshow(target[i].permute(1, 2, 0) * 0.5 + 0.5)\n        axs[i, 1].set_title('Real Image')\n        axs[i, 1].axis('off')\n\n        axs[i, 2].imshow(prediction[i].permute(1, 2, 0) * 0.5 + 0.5)\n        axs[i, 2].set_title('Generated Image')\n        axs[i, 2].axis('off')\n\n    plt.suptitle(f'Epoch {epoch}')\n    if display:\n        plt.show()\n    plt.close()\n\n    generator.train()  # Set the generator back to training mode\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.659069Z","iopub.execute_input":"2024-05-07T22:20:14.659761Z","iopub.status.idle":"2024-05-07T22:20:14.677456Z","shell.execute_reply.started":"2024-05-07T22:20:14.659717Z","shell.execute_reply":"2024-05-07T22:20:14.675943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as TF\n\nclass ResizeMaintainAspectRatio:\n    def __init__(self, target_size):\n        \"\"\"\n        target_size (int): The target size for the smallest dimension of the image.\n        \"\"\"\n        self.target_size = target_size\n\n    def __call__(self, image):\n        # Compute the scaling factor to maintain aspect ratio.\n        width, height = image.size\n        min_dim = min(width, height)\n        scaling_factor = self.target_size / min_dim\n        \n        new_width = max(256,int(width * scaling_factor))\n        new_height = max(256,int(height * scaling_factor))\n\n        # Resize the image with the computed dimensions\n        image = TF.resize(image, (new_height, new_width))\n        return image\n\nclass SplitAndCropTransform:\n    def __init__(self, output_size, resize_transform=None):\n        \"\"\"\n        output_size (tuple): Desired output size of the crop, e.g., (256, 256).\n        resize_transform (callable): Transformation to resize the image.\n        \"\"\"\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            self.output_size = output_size\n        self.resize_transform = resize_transform\n\n    def __call__(self, image):\n\n\n        if self.resize_transform:\n            image = self.resize_transform(image)\n\n        i, j, h, w = transforms.RandomCrop.get_params(\n            image, output_size=self.output_size)\n\n        image = TF.crop(image, i, j, h, w)\n\n        return image\n\n# Example of using the transformations\nresize_transform = ResizeMaintainAspectRatio(256)\nsplit_transform = SplitAndCropTransform((256, 256), resize_transform=resize_transform)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.679593Z","iopub.execute_input":"2024-05-07T22:20:14.680136Z","iopub.status.idle":"2024-05-07T22:20:14.699601Z","shell.execute_reply.started":"2024-05-07T22:20:14.680088Z","shell.execute_reply":"2024-05-07T22:20:14.698094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torchvision import transforms\n# from PIL import Image\n\n# transform = transforms.Compose([\n#     transforms.ToTensor(),  # Convert the PIL image to a PyTorch tensor\n# ])\n# for img_path in img_fns:\n#     try:\n#         image_id = img_path.split('/')[-1].split('.')[0]\n\n#         # Load image\n#         image = Image.open(img_path)\n#         image = split_transform(image)\n#         image_tensor = transform(image)\n#         image_tensor = image_tensor.unsqueeze(0)\n        \n#         # Generate images (replace `image` twice with the actual mask and image if needed)\n#         generate_images(generator, image_tensor, image_tensor, 1, display=True)\n        \n#     except Exception as e:\n#         print(f\"Error processing {img_path}: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.701134Z","iopub.execute_input":"2024-05-07T22:20:14.701583Z","iopub.status.idle":"2024-05-07T22:20:14.718619Z","shell.execute_reply.started":"2024-05-07T22:20:14.701548Z","shell.execute_reply":"2024-05-07T22:20:14.717305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_mosaic(image, bbox, pixelation_level=10):\n    \"\"\"\n    Applies a mosaic effect to a specified bounding box within an image.\n    \n    Args:\n    image (PIL.Image): The original image.\n    bbox (tuple): A tuple of (x, y, width, height) for the bounding box.\n    pixelation_level (int): The size of each mosaic block.\n    \n    Returns:\n    PIL.Image: Image with mosaic applied to the region.\n    \"\"\"\n    \n    \n    # Unpack the bounding box\n    x, y, w, h = map(int, bbox)\n    pixelation_level = max(min(w//3,h//3),1)\n#     print(\"w\" + str(w))\n#     print(\"h\" + str(h))\n    \n    # Crop the relevant part of the image\n    cropped_img = image.crop((x, y, x + w, y + h))\n    \n#     print(\"-1\")\n    \n    # Resize the cropped image to cause the pixelation effect\n    cropped_img = cropped_img.resize(\n        (w // pixelation_level, h // pixelation_level), \n        resample=Image.BOX\n    )\n#     print(\"-2\")\n    \n    # Resize back to original size\n    cropped_img = cropped_img.resize(\n        (w, h),\n        Image.NEAREST\n    )\n#     print(\"-3\")\n    # Paste the pixelated region back to the original image\n    image.paste(cropped_img, (x, y))\n    \n    return image\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.720283Z","iopub.execute_input":"2024-05-07T22:20:14.721108Z","iopub.status.idle":"2024-05-07T22:20:14.733847Z","shell.execute_reply.started":"2024-05-07T22:20:14.721052Z","shell.execute_reply":"2024-05-07T22:20:14.732434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image, ImageDraw\n\ndef apply_white_blank(image, bbox):\n    \"\"\"\n    Applies a white blank to a specified bounding box within an image.\n\n    Args:\n    image (PIL.Image): The original image.\n    bbox (tuple): A tuple of (x, y, width, height) for the bounding box.\n\n    Returns:\n    PIL.Image: Image with a white blank applied to the region.\n    \"\"\"\n\n    # Unpack the bounding box\n    x, y, w, h = map(int, bbox)\n\n    # Create a white rectangle\n    draw = ImageDraw.Draw(image)\n    draw.rectangle([x, y, x + w, y + h], fill=\"white\")\n\n    return image\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.735606Z","iopub.execute_input":"2024-05-07T22:20:14.736000Z","iopub.status.idle":"2024-05-07T22:20:14.744048Z","shell.execute_reply.started":"2024-05-07T22:20:14.735964Z","shell.execute_reply":"2024-05-07T22:20:14.743009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n# import numpy as np\n# import torch\n\n# def draw_random_wavy_lines(image_shape, num_lines=10, thickness=3, max_step=5):\n#     print(\"2\")\n#     \"\"\"Generate a mask with wavy lines drawn.\"\"\"\n#     H, W = image_shape\n#     mask = np.zeros((H, W), dtype=bool)\n#     for _ in range(num_lines):\n#         line = random_wavy_line(H, W, max_step, thickness)\n#         mask = mask | line\n#     return torch.tensor(mask)\n\n# def random_wavy_line(H, W, max_step, thickness):\n#     \"\"\"Generate a wavy line mask with random walk behavior.\"\"\"\n#     print(\"3\")\n#     y, x = np.ogrid[:H, :W]\n#     line_mask = np.zeros((H, W), dtype=bool)\n\n#     x1, y1 = np.random.randint(0, W), np.random.randint(0, H)\n\n#     for _ in range(100):  # Adjust the number of iterations for longer lines\n#         x2 = x1 + np.random.randint(-max_step, max_step)\n#         y2 = y1 + np.random.randint(-max_step, max_step)\n\n#         x2 = np.clip(x2, 0, W - 1)\n#         y2 = np.clip(y2, 0, H - 1)\n\n#         line_segment = bresenham_line(y1, x1, y2, x2, H, W, thickness)\n#         line_mask = line_mask | line_segment\n\n#         x1, y1 = x2, y2\n\n#     return line_mask\n\n# def bresenham_line(y1, x1, y2, x2, H, W, thickness=1):\n#     \"\"\"Generate a mask for a line using Bresenham's line algorithm.\"\"\"\n#     y, x = np.ogrid[:H, :W]\n#     line_mask = np.zeros((H, W), dtype=bool)\n\n#     dx = abs(x2 - x1)\n#     dy = -abs(y2 - y1)\n#     sx = 1 if x1 < x2 else -1\n#     sy = 1 if y1 < y2 else -1\n#     err = dx + dy\n\n#     while True:\n#         brush = (x - x1)**2 + (y - y1)**2 <= (thickness / 2) ** 2\n#         line_mask = line_mask | brush\n\n#         if x1 == x2 and y1 == y2:\n#             break\n#         e2 = 2 * err\n#         if e2 >= dy:\n#             err += dy\n#             x1 += sx\n#         if e2 <= dx:\n#             err += dx\n#             y1 += sy\n\n#     return line_mask\n\n# def apply_white_doodle(image, num_lines=5, thickness=20, max_step=5):\n#     print(\"1\")\n#     # Open the image and convert it to a NumPy array\n#     image_np  = np.array(image)\n\n#     # Create the mask\n#     mask = draw_random_wavy_lines(image_np.shape[:2], num_lines, thickness, max_step).numpy()\n\n#     # Apply the mask (assuming the image is in RGB)\n#     image_np[mask] = [255, 255, 255]\n\n#     # Convert the modified array back to a PIL image\n#     image_modified = Image.fromarray(image_np)\n\n#     return image_modified\n\n# # Example usage:\n# # result_image = apply_white_doodle('path/to/your/image.jpg')\n# # result_image.show() or result_image.save('output.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.745663Z","iopub.execute_input":"2024-05-07T22:20:14.746421Z","iopub.status.idle":"2024-05-07T22:20:14.760272Z","shell.execute_reply.started":"2024-05-07T22:20:14.746379Z","shell.execute_reply":"2024-05-07T22:20:14.758699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom PIL import Image\n\ndef random_wavy_lines_opencv(H, W, max_step, num_points=50, thickness=2):\n    \"\"\"Generate a wavy line mask using OpenCV.\"\"\"\n    # Initialize the starting point\n    x1, y1 = np.random.randint(0, W), np.random.randint(0, H)\n    points = [(x1, y1)]\n\n    # Generate a random walk to determine the next points\n    for _ in range(num_points):\n        x2 = x1 + np.random.randint(-max_step, max_step)\n        y2 = y1 + np.random.randint(-max_step, max_step)\n\n        # Keep within image boundaries\n        x2 = np.clip(x2, 0, W - 1)\n        y2 = np.clip(y2, 0, H - 1)\n        points.append((x2, y2))\n\n        x1, y1 = x2, y2\n\n    # Draw the wavy line on a mask\n    mask = np.zeros((H, W, 3), dtype=np.uint8)\n    cv2.polylines(mask, [np.array(points)], isClosed=False, color=(255, 255, 255), thickness=thickness)\n\n    return mask\n\ndef apply_white_doodle_opencv(image, num_lines=10, thickness=2, max_step=15):\n    \"\"\"Apply white doodles on the image using OpenCV.\"\"\"\n    # Open the image and convert it to a NumPy array\n    image_np = np.array(image)\n\n    H, W = image_np.shape[:2]\n    for _ in range(num_lines):\n        mask = random_wavy_lines_opencv(H, W, max_step, thickness=thickness)\n        image_np[mask[:, :, 0] == 255] = [255, 255, 255]\n\n    # Convert back to PIL\n    image_modified = Image.fromarray(image_np)\n\n    return image_modified\n\n# Example usage:\n# image = Image.open('path/to/your/image.jpg')\n# result_image = apply_white_doodle_opencv(image)\n# result_image.show() or result_image.save('output.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:14.764121Z","iopub.execute_input":"2024-05-07T22:20:14.764639Z","iopub.status.idle":"2024-05-07T22:20:15.015127Z","shell.execute_reply.started":"2024-05-07T22:20:14.764582Z","shell.execute_reply":"2024-05-07T22:20:15.013718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef calculate_expanded_bbox(annotations, img_size):\n    \"\"\"\n    Calculates a single bounding box that encompasses all specified bounding boxes and expands it.\n    \n    Args:\n    annotations (pd.DataFrame): DataFrame containing 'bbox' columns with format (x, y, w, h).\n    img_size (tuple): The size (width, height) of the image.\n    \n    Returns:\n    tuple: A tuple (x, y, w, h) representing the expanded bounding box.\n    \"\"\"\n    # Initialize min and max coordinates with extreme values\n    min_x, min_y = float('inf'), float('inf')\n    max_x, max_y = 0, 0\n    area = 0\n    # Iterate through annotations to find the extreme points\n    for _, row in annotations.iterrows():\n        x, y, w, h = row['bbox']\n        area = area + w*h\n        min_x = min(min_x, x)\n        min_y = min(min_y, y)\n        max_x = max(max_x, x + w)\n        max_y = max(max_y, y + h)\n    \n\n    # Calculate the center, width, and height of the bounding box\n    center_x = (min_x + max_x) / 2\n    center_y = (min_y + max_y) / 2\n    width = max_x - min_x\n    height = max_y - min_y\n    \n    # Expand the box by 2 centered on the original center\n    \n    new_width = min(img_size[0], 2 * width)\n    new_height = min(img_size[1], 2 * height)\n    \n    # Calculate the new top-left coordinates, ensuring the box stays within the image bounds\n    new_x = max(0, center_x - new_width / 2)\n    new_y = max(0, center_y - new_height / 2)\n    \n    # Ensure the bottom-right corner does not go out of bounds\n    new_width = min(img_size[0]-new_x, new_width)\n    new_height = max(min(img_size[1] - new_y, new_height),256)\n    \n    if(new_width<256):\n        if(new_x<128):\n            new_x = 0\n        else:\n            new_x = new_x - random.randint(0, 128)\n        new_width = 256\n        \n    if(new_height<256):\n        if(new_y<128):\n            new_y = 0\n        else:\n            new_y = new_y - random.randint(0, 128)\n        new_height = 256\n        \n    \n    if(int(new_x) + int(new_width)>img_size[0] or int(new_y) + int(new_height)>img_size[1]):\n        raise ValueError(\"too large\")\n\n    percentage = area/(int(new_width)*int(new_height))\n    if(percentage<0.01):\n        raise ValueError(\"too small\")\n    \n    \n    return (int(new_x), int(new_y), int(new_width), int(new_height))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:15.016842Z","iopub.execute_input":"2024-05-07T22:20:15.017333Z","iopub.status.idle":"2024-05-07T22:20:15.043358Z","shell.execute_reply.started":"2024-05-07T22:20:15.017288Z","shell.execute_reply":"2024-05-07T22:20:15.041834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\ndef weights_init(init_type='gaussian'):\n    def init_fun(m):\n        classname = m.__class__.__name__\n        if (classname.find('Conv') == 0 or classname.find(\n                'Linear') == 0) and hasattr(m, 'weight'):\n            if init_type == 'gaussian':\n                nn.init.normal_(m.weight, 0.0, 0.02)\n            elif init_type == 'xavier':\n                nn.init.xavier_normal_(m.weight, gain=math.sqrt(2))\n            elif init_type == 'kaiming':\n                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))\n            elif init_type == 'default':\n                pass\n            else:\n                assert 0, \"Unsupported initialization: {}\".format(init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n    return init_fun\n\n\nclass VGG16FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg16 = models.vgg16(pretrained=True)\n        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n\n        # fix the encoder\n        for i in range(3):\n            for param in getattr(self, 'enc_{:d}'.format(i + 1)).parameters():\n                param.requires_grad = False\n\n    def forward(self, image):\n        results = [image]\n        for i in range(3):\n            func = getattr(self, 'enc_{:d}'.format(i + 1))\n            results.append(func(results[-1]))\n        return results[1:]\n\n\nclass PartialConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super().__init__()\n        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                                    stride, padding, dilation, groups, bias)\n        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                                   stride, padding, dilation, groups, False)\n        self.input_conv.apply(weights_init('kaiming'))\n\n        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n\n        # mask is not updated\n        for param in self.mask_conv.parameters():\n            param.requires_grad = False\n\n    def forward(self, input, mask):\n        # http://masc.cs.gmu.edu/wiki/partialconv\n        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n        # W^T* (M .* X) / sum(M) + b = [C(M .* X) â€“ C(0)] / D(M) + C(0)\n\n        output = self.input_conv(input * mask)\n        if self.input_conv.bias is not None:\n            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(\n                output)\n        else:\n            output_bias = torch.zeros_like(output)\n\n        with torch.no_grad():\n            output_mask = self.mask_conv(mask)\n\n        no_update_holes = output_mask == 0\n        mask_sum = output_mask.masked_fill_(no_update_holes, 1.0)\n\n        output_pre = (output - output_bias) / mask_sum + output_bias\n        output = output_pre.masked_fill_(no_update_holes, 0.0)\n\n        new_mask = torch.ones_like(output)\n        new_mask = new_mask.masked_fill_(no_update_holes, 0.0)\n\n        return output, new_mask\n\n\nclass PCBActiv(nn.Module):\n    def __init__(self, in_ch, out_ch, bn=True, sample='none-3', activ='relu',\n                 conv_bias=False):\n        super().__init__()\n        if sample == 'down-5':\n            self.conv = PartialConv(in_ch, out_ch, 5, 2, 2, bias=conv_bias)\n        elif sample == 'down-7':\n            self.conv = PartialConv(in_ch, out_ch, 7, 2, 3, bias=conv_bias)\n        elif sample == 'down-3':\n            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias)\n        else:\n            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias)\n\n        if bn:\n            self.bn = nn.BatchNorm2d(out_ch)\n        if activ == 'relu':\n            self.activation = nn.ReLU()\n        elif activ == 'leaky':\n            self.activation = nn.LeakyReLU(negative_slope=0.2)\n\n    def forward(self, input, input_mask):\n        h, h_mask = self.conv(input, input_mask)\n        if hasattr(self, 'bn'):\n            h = self.bn(h)\n        if hasattr(self, 'activation'):\n            h = self.activation(h)\n        return h, h_mask\n\n\nclass PConvUNet(nn.Module):\n    def __init__(self, layer_size=7, input_channels=3, upsampling_mode='nearest'):\n        super().__init__()\n        self.freeze_enc_bn = False\n        self.upsampling_mode = upsampling_mode\n        self.layer_size = layer_size\n        self.enc_1 = PCBActiv(input_channels, 64, bn=False, sample='down-7')\n        self.enc_2 = PCBActiv(64, 128, sample='down-5')\n        self.enc_3 = PCBActiv(128, 256, sample='down-5')\n        self.enc_4 = PCBActiv(256, 512, sample='down-3')\n        for i in range(4, self.layer_size):\n            name = 'enc_{:d}'.format(i + 1)\n            setattr(self, name, PCBActiv(512, 512, sample='down-3'))\n\n        for i in range(4, self.layer_size):\n            name = 'dec_{:d}'.format(i + 1)\n            setattr(self, name, PCBActiv(512 + 512, 512, activ='leaky'))\n        self.dec_4 = PCBActiv(512 + 256, 256, activ='leaky')\n        self.dec_3 = PCBActiv(256 + 128, 128, activ='leaky')\n        self.dec_2 = PCBActiv(128 + 64, 64, activ='leaky')\n        self.dec_1 = PCBActiv(64 + input_channels, input_channels,\n                              bn=False, activ=None, conv_bias=True)\n\n    def forward(self, input, input_mask):\n        h_dict = {}  # for the output of enc_N\n        h_mask_dict = {}  # for the output of enc_N\n\n        h_dict['h_0'], h_mask_dict['h_0'] = input, input_mask\n\n        h_key_prev = 'h_0'\n        for i in range(1, self.layer_size + 1):\n            l_key = 'enc_{:d}'.format(i)\n            h_key = 'h_{:d}'.format(i)\n            h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(\n                h_dict[h_key_prev], h_mask_dict[h_key_prev])\n            h_key_prev = h_key\n\n        h_key = 'h_{:d}'.format(self.layer_size)\n        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n\n        # concat upsampled output of h_enc_N-1 and dec_N+1, then do dec_N\n        # (exception)\n        #                            input         dec_2            dec_1\n        #                            h_enc_7       h_enc_8          dec_8\n\n        for i in range(self.layer_size, 0, -1):\n            enc_h_key = 'h_{:d}'.format(i - 1)\n            dec_l_key = 'dec_{:d}'.format(i)\n\n            h = F.interpolate(h, scale_factor=2, mode=self.upsampling_mode)\n            h_mask = F.interpolate(\n                h_mask, scale_factor=2, mode='nearest')\n\n            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim=1)\n            h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n\n        return h, h_mask\n\n    def train(self, mode=True):\n        \"\"\"\n        Override the default train() to freeze the BN parameters\n        \"\"\"\n        super().train(mode)\n        if self.freeze_enc_bn:\n            for name, module in self.named_modules():\n                if isinstance(module, nn.BatchNorm2d) and 'enc' in name:\n                    module.eval()\n\n\n# if __name__ == '__main__':\n#     size = (1, 3, 5, 5)\n#     input = torch.ones(size)\n#     input_mask = torch.ones(size)\n#     input_mask[:, :, 2:, :][:, :, :, 2:] = 0\n\n#     conv = PartialConv(3, 3, 3, 1, 1)\n#     l1 = nn.L1Loss()\n#     input.requires_grad = True\n\n#     output, output_mask = conv(input, input_mask)\n#     loss = l1(output, torch.randn(1, 3, 5, 5))\n#     loss.backward()\n\n#     assert (torch.sum(input.grad != input.grad).item() == 0)\n#     assert (torch.sum(torch.isnan(conv.input_conv.weight.grad)).item() == 0)\n#     assert (torch.sum(torch.isnan(conv.input_conv.bias.grad)).item() == 0)\n\n#     # model = PConvUNet()\n#     # output, output_mask = model(input, input_mask)\nmodel = PConvUNet()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:15.045721Z","iopub.execute_input":"2024-05-07T22:20:15.046603Z","iopub.status.idle":"2024-05-07T22:20:15.752438Z","shell.execute_reply.started":"2024-05-07T22:20:15.046520Z","shell.execute_reply":"2024-05-07T22:20:15.751408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n\n\ndef load_checkpoint(filename):\n    checkpoint = torch.load(filename,map_location=torch.device('cpu'))\n    model.load_state_dict(checkpoint['model_state'])\n    epoch = checkpoint['epoch']\n    return epoch\n\n# Load the checkpoint\ncheckpoint_filename = '/kaggle/input/checkpointforpartinalconvepoch40/checkpointWithWhiteLines_epoch_40.pth'  # Replace X with the actual epoch number\nstart_epoch = load_checkpoint(checkpoint_filename)\n\n# Now the model and optimizer have the state loaded from the checkpoint\nprint(f'Checkpoint loaded, starting from epoch {start_epoch}')","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:15.756924Z","iopub.execute_input":"2024-05-07T22:20:15.757464Z","iopub.status.idle":"2024-05-07T22:20:20.165785Z","shell.execute_reply.started":"2024-05-07T22:20:15.757414Z","shell.execute_reply":"2024-05-07T22:20:20.164269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output_directory = 'cGAN_images_with_white'\n# os.makedirs(output_directory, exist_ok=True)\n\n# import torch\n# import numpy as np\n# import torchvision.transforms as transforms\n# from PIL import Image\n# import matplotlib.pyplot as plt\n# import os\n\n\n# transform = transforms.Compose([\n#     transforms.ToTensor(),  # Convert the PIL image to a PyTorch tensor\n# ])\n\n# def tensor_to_pil(tensor):\n#     tensor = tensor.detach().cpu()  # Detach from gradients and move to CPU\n#     array = tensor.permute(1, 2, 0).numpy()  # Convert to HWC format\n#     array = array * 255  # Denormalize\n#     array = array.clip(0, 255).astype(np.uint8)  # Clip and convert to uint8\n#     return Image.fromarray(array)\n\n\n# for img_path in img_fns:\n#     try:\n#         image_id = img_path.split('/')[-1].split('.')[0]\n\n#         # Load image\n#         image = Image.open(img_path)\n\n#         # Filter annotations for this image\n#         relevant_annots = annot.query('image_id == @image_id')\n\n#         if not relevant_annots.empty:\n#             x_c, y_c, w_c, h_c = calculate_expanded_bbox(relevant_annots, image.size)\n\n# #             # Process each bbox\n# #             for _, row in relevant_annots.iterrows():\n# #                 #image = apply_mosaic(image, row['bbox'])\n# #                 image = apply_white_blank(image, row['bbox'])\n# #             #image = apply_white_doodle(image)\n\n#             # Crop the mosaic image\n#             image = image.crop((x_c, y_c, x_c + w_c, y_c + h_c))\n# #             image = split_transform(image)\n#             original = image.copy()\n            \n# #             image = transform(image)\n# #             image = image.unsqueeze(0)\n# #             with torch.no_grad():\n# #                 image = generator(image)\n\n# #             image = image[0]\n# #             image = tensor_to_pil(image)\n            \n#             image = apply_white_doodle_opencv(image)\n# #             image = transform(image)\n# #             image = image.unsqueeze(0)\n# #             with torch.no_grad():\n# #                 image = model(image)\n# #             image = image[0]\n# #             image = tensor_to_pil(image)\n            \n            \n\n\n# #             # Crop the original image to the same region\n# #             original = Image.open(img_path)\n# #             original = original.crop((x_c, y_c, x_c + w_c, y_c + h_c))\n\n#             # Create a new image by combining the original and the processed\n#             combined_image = Image.new('RGB', (original.width * 2, original.height))\n#             combined_image.paste(original, (0, 0))\n#             combined_image.paste(image, (original.width, 0))\n\n# #             # Display the image\n# #             plt.imshow(combined_image)\n# #             plt.axis('off')\n# #             plt.show()\n\n#             # Save the combined image\n#             combined_image.save(os.path.join(output_directory, f'{image_id}_combined.jpg'))\n\n#     except Exception as e:\n#         print(f\"An error occurred while processing {img_path}: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:20.167883Z","iopub.execute_input":"2024-05-07T22:20:20.168303Z","iopub.status.idle":"2024-05-07T22:20:20.176618Z","shell.execute_reply.started":"2024-05-07T22:20:20.168266Z","shell.execute_reply":"2024-05-07T22:20:20.175137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:20.178767Z","iopub.execute_input":"2024-05-07T22:20:20.179148Z","iopub.status.idle":"2024-05-07T22:20:20.218836Z","shell.execute_reply.started":"2024-05-07T22:20:20.179115Z","shell.execute_reply":"2024-05-07T22:20:20.217081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_directory = 'cGAN_images_with_white'\nos.makedirs(output_directory, exist_ok=True)\n\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert the PIL image to a PyTorch tensor\n])\n\ndef tensor_to_pil(tensor):\n    tensor = tensor.detach().cpu()  # Detach from gradients and move to CPU\n    array = tensor.permute(1, 2, 0).numpy()  # Convert to HWC format\n    array = array * 255  # Denormalize\n    array = array.clip(0, 255).astype(np.uint8)  # Clip and convert to uint8\n    return Image.fromarray(array)\n\n\nfor img_path in img_fns:\n    try:\n        image_id = img_path.split('/')[-1].split('.')[0]\n\n        # Load image\n        image = Image.open(img_path)\n\n        # Filter annotations for this image\n        relevant_annots = annot.query('image_id == @image_id')\n\n        if not relevant_annots.empty:\n            x_c, y_c, w_c, h_c = calculate_expanded_bbox(relevant_annots, image.size)\n\n#             # Process each bbox\n#             for _, row in relevant_annots.iterrows():\n#                 #image = apply_mosaic(image, row['bbox'])\n#                 image = apply_white_blank(image, row['bbox'])\n                \n            \n            # Crop the mosaic image\n            image = image.crop((x_c, y_c, x_c + w_c, y_c + h_c))\n            \n            image = split_transform(image)\n            \n            original = image.copy()\n                \n\n            \n            image = transform(image)\n#             print(image.shape)\n            image = image.unsqueeze(0)\n#             print(image.shape)\n#             print(\"2\")\n            with torch.no_grad():\n#                 print(\"3\")\n                image = generator(image)\n#                 print(\"0\")\n                \n#             print(\"1\")\n            image = image[0]\n            \n            image = tensor_to_pil(image)\n\n            image = apply_white_doodle_opencv(image)\n\n\n            \n\n            # Crop the original image to the same region\n#             original = Image.open(img_path)\n#             original = original.crop((x_c, y_c, x_c + w_c, y_c + h_c))\n\n            # Create a new image by combining the original and the processed\n            combined_image = Image.new('RGB', (original.width * 2, original.height))\n            combined_image.paste(original, (0, 0))\n            combined_image.paste(image, (original.width, 0))\n\n#             Display the image\n            plt.imshow(combined_image)\n            plt.axis('off')\n            plt.show()\n\n            # Save the combined image\n            combined_image.save(os.path.join(output_directory, f'{image_id}_combined.jpg'))\n\n    except Exception as e:\n        print(f\"An error occurred while processing {img_path}: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:20.220687Z","iopub.execute_input":"2024-05-07T22:20:20.222580Z","iopub.status.idle":"2024-05-07T22:20:36.447328Z","shell.execute_reply.started":"2024-05-07T22:20:20.222522Z","shell.execute_reply":"2024-05-07T22:20:36.445500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\n\ndef zip_directory(folder_path, output_path):\n    \"\"\"\n    Zips the contents of the specified folder.\n\n    Args:\n    folder_path (str): The path to the folder to zip.\n    output_path (str): The path where the zip file will be saved.\n    \"\"\"\n    # Create a ZipFile object in 'write' mode\n    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through directory\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Create the full path of the file\n                full_path = os.path.join(root, file)\n                # Add file to zip\n                zipf.write(full_path, arcname=os.path.relpath(full_path, folder_path))\n\noutput_directory = 'cGAN_images_with_white'\nzip_output_path = 'processed_images.zip'\n\n# First, we ensure all processing is done and saved to 'output_directory'\n# After processing all images and saving them:\nzip_directory(output_directory, zip_output_path)\nprint(f'All files zipped successfully into {zip_output_path}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:20:36.448665Z","iopub.status.idle":"2024-05-07T22:20:36.449161Z","shell.execute_reply.started":"2024-05-07T22:20:36.448934Z","shell.execute_reply":"2024-05-07T22:20:36.448957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}